{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16fc4cc",
   "metadata": {},
   "source": [
    "# Basic LLM Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfce72",
   "metadata": {},
   "source": [
    "## Step 1: Text Tokenization\n",
    "We define a small vocabulary and tokenize a sentence into numbers (token IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5057f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[7, 8, 5],\n",
      "        [0, 1, 2]])\n",
      "Target: tensor([[8, 5, 6],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define a toy vocabulary\n",
    "vocab = {\"every\": 0, \"effort\": 1, \"moves\": 2, \"you\": 3, \"END\": 4, \"like\": 5, \"chocolate\": 6, \"I\": 7, \"really\": 8}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Training data: inputs and targets (shifted by one token)\n",
    "train_sentences = [[7, 8, 5], [0, 1, 2]]  # [\"I really like\"], [\"every effort moves\"]\n",
    "train_targets = [[8, 5, 6], [1, 2, 3]]   # [\"really like chocolate\"], [\"effort moves you\"]\n",
    "\n",
    "# Convert to tensors\n",
    "x_train = torch.tensor(train_sentences)\n",
    "y_train = torch.tensor(train_targets)\n",
    "\n",
    "print(\"Input:\", x_train)\n",
    "print(\"Target:\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09edf449",
   "metadata": {},
   "source": [
    "## Step 2: Loss Function (Cross Entropy)\n",
    "We use a standard loss to measure how different the model's output is from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3826f049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.4884560108184814\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny model with embedding + linear layer\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "# Forward pass (generate predictions)\n",
    "embedded = embedding(x_train)              # Shape: [2, 3, 10]\n",
    "logits = linear(embedded)                  # Shape: [2, 3, vocab_size]\n",
    "\n",
    "# Flatten for cross entropy loss\n",
    "logits_flat = logits.view(-1, vocab_size)  # [6, vocab_size]\n",
    "targets_flat = y_train.view(-1)            # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Initial loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9050c",
   "metadata": {},
   "source": [
    "## Step 3: Training Loop\n",
    "Train the model for a few epochs and update weights to reduce loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "719bb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.4885\n",
      "Epoch 2, Loss: 1.9622\n",
      "Epoch 3, Loss: 1.5239\n",
      "Epoch 4, Loss: 1.1625\n",
      "Epoch 5, Loss: 0.8660\n",
      "Epoch 6, Loss: 0.6258\n",
      "Epoch 7, Loss: 0.4361\n",
      "Epoch 8, Loss: 0.2924\n",
      "Epoch 9, Loss: 0.1897\n",
      "Epoch 10, Loss: 0.1205\n"
     ]
    }
   ],
   "source": [
    "# Optimizer to update model weights\n",
    "optimizer = torch.optim.AdamW(list(embedding.parameters()) + list(linear.parameters()), lr=0.05)\n",
    "\n",
    "# Run a simple training loop\n",
    "for epoch in range(10):\n",
    "    embedded = embedding(x_train)\n",
    "    logits = linear(embedded)\n",
    "    logits_flat = logits.view(-1, vocab_size)\n",
    "    targets_flat = y_train.view(-1)\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd048dc3",
   "metadata": {},
   "source": [
    "## Step 4: Validation Set\n",
    "We test the model on data it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3d31bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.09278151392936707\n"
     ]
    }
   ],
   "source": [
    "# Validation data\n",
    "val_input = torch.tensor([[7, 8, 5]])  # \"I really like\"\n",
    "val_target = torch.tensor([[8, 5, 6]])  # \"really like chocolate\"\n",
    "\n",
    "# Run model\n",
    "with torch.no_grad():\n",
    "    val_emb = embedding(val_input)\n",
    "    val_logits = linear(val_emb)\n",
    "    val_loss = torch.nn.functional.cross_entropy(val_logits.view(-1, vocab_size), val_target.view(-1))\n",
    "\n",
    "print(\"Validation Loss:\", val_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe48379",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation (Prediction)\n",
    "We predict the next word for a given prompt and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7953c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: you\n"
     ]
    }
   ],
   "source": [
    "# Predict next token after the prompt \"every effort moves\"\n",
    "prompt = torch.tensor([[0, 1, 2]])  # \"every effort moves\"\n",
    "with torch.no_grad():\n",
    "    emb = embedding(prompt)\n",
    "    logits = linear(emb)\n",
    "    next_token_logits = logits[0, -1]  # last token's prediction\n",
    "    predicted_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "print(\"Predicted next word:\", inv_vocab[predicted_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37dc4cb",
   "metadata": {},
   "source": [
    "## Step 6: Sample Generation\n",
    "Generate a short sentence word-by-word from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f7978f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: I really like chocolate like chocolate\n"
     ]
    }
   ],
   "source": [
    "# Start with \"I\"\n",
    "sentence = [7]  # 'I'\n",
    "\n",
    "# Generate 5 more words\n",
    "for _ in range(5):\n",
    "    input_tensor = torch.tensor([sentence])\n",
    "    with torch.no_grad():\n",
    "        emb = embedding(input_tensor)\n",
    "        logits = linear(emb)\n",
    "        next_token_logits = logits[0, -1]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        sentence.append(next_token)\n",
    "        if next_token == vocab['END']:\n",
    "            break\n",
    "\n",
    "# Decode sentence\n",
    "decoded = [inv_vocab[tok] for tok in sentence]\n",
    "print(\"Generated sentence:\", ' '.join(decoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
