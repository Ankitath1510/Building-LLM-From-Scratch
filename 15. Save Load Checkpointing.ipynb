{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2097f6",
   "metadata": {},
   "source": [
    "# Save & Load an LLM (Checkpointing)  \n",
    "\n",
    "We’ll:\n",
    "1. Train a tiny toy \"model\" (just a weight and bias) so we have something to save.\n",
    "2. **Save only the model weights**.\n",
    "3. **Load** those weights into fresh parameters.\n",
    "4. **Save model + optimizer** together (best practice for resuming).\n",
    "5. **Load and resume** training from that checkpoint.\n",
    "6. Show how to make runs more **deterministic** (less randomness).\n",
    "\n",
    "> Runs on **CPU**. No GPU required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcd549",
   "metadata": {},
   "source": [
    "## 0) Setup (imports + seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b451d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and set a random seed so results are reproducible\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(123)  # Make random numbers repeatable for this demo\n",
    "device = \"cpu\"          # Force CPU usage to keep it simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ab451",
   "metadata": {},
   "source": [
    "## 1) A tiny toy dataset (so we can demonstrate saving/loading)\n",
    "We pretend the true relationship is `y = 3x + 2`.  \n",
    "We'll let a tiny \"model\" learn that (just a weight `w` and bias `b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple inputs (x) and targets (y_true)\n",
    "x_batch = torch.linspace(0, 1, steps=16).unsqueeze(1)  # 16 inputs between 0 and 1, shape [16,1]\n",
    "y_true  = 3.0 * x_batch + 2.0                          # True outputs for our demo\n",
    "print(x_batch[:5].flatten(), \"->\", y_true[:5].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07242f7d",
   "metadata": {},
   "source": [
    "## 2) A tiny \"model\" (no classes) + Optimizer + Loss\n",
    "Think of this as the LLM's brain in miniature — just two parameters (`w` and `b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model's parameters randomly\n",
    "w = torch.randn(1, requires_grad=True)   # model weight\n",
    "b = torch.randn(1, requires_grad=True)   # model bias\n",
    "\n",
    "# AdamW is our \"nutritionist/coach\" that gently updates w and b\n",
    "optimizer = optim.AdamW([w, b], lr=1e-2, weight_decay=0.01)\n",
    "\n",
    "# We'll measure error using Mean Squared Error (how far our guess is)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"Initial w,b:\", w.item(), b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bd444",
   "metadata": {},
   "source": [
    "## 3) Quick training  \n",
    "We do a few steps so the model learns something worth saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31347f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a handful of steps — predict -> compute loss -> backprop -> update\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()          # Clear old gradient info (yesterday's mistakes)\n",
    "    y_pred = x_batch * w + b       # Model's current guess (y = w*x + b)\n",
    "    loss = loss_fn(y_pred, y_true) # How wrong is the guess?\n",
    "    loss.backward()                # Compute feedback (gradients)\n",
    "    optimizer.step()               # Update w and b a little bit\n",
    "\n",
    "print(\"After training — w,b:\", w.item(), b.item())\n",
    "print(\"Training loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb1620",
   "metadata": {},
   "source": [
    "## 4) Save **only** the model weights (like saving the chef's skills)\n",
    "In a real `nn.Module` you'd save `state_dict()`.  \n",
    "Here, since we have no class, we **pack the tensors ourselves**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27401614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple \"state dict\" manually and save to disk\n",
    "model_state = {\n",
    "    \"w\": w.detach().clone(),   # store learned weight\n",
    "    \"b\": b.detach().clone()    # store learned bias\n",
    "}\n",
    "\n",
    "torch.save(model_state, \"model.pth\")\n",
    "print(\"Saved model weights to: model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36abf2",
   "metadata": {},
   "source": [
    "## 5) Load the model weights into **fresh** parameters  \n",
    "Imagine a new session - we start with random params and **load** the saved ones in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with random parameters (as if in a new Python session)\n",
    "w_loaded = torch.randn(1, requires_grad=True)\n",
    "b_loaded = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Load from disk and copy into these new parameters\n",
    "checkpoint = torch.load(\"model.pth\", map_location=device)\n",
    "with torch.no_grad():\n",
    "    w_loaded.copy_(checkpoint[\"w\"])\n",
    "    b_loaded.copy_(checkpoint[\"b\"])\n",
    "\n",
    "print(\"Loaded w,b:\", w_loaded.item(), b_loaded.item())\n",
    "\n",
    "# Conceptual 'eval' mode: in a real Module you'd call model.eval() to disable dropout, etc.\n",
    "is_eval_mode = True\n",
    "print(\"Eval mode flag:\", is_eval_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e1159",
   "metadata": {},
   "source": [
    "## 6) Save **model + optimizer** (best practice for resuming training)\n",
    "This keeps the optimizer's internal state (momentum, etc.) so training can continue smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94781dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_state = {\n",
    "    \"model\": {\"w\": w.detach().clone(), \"b\": b.detach().clone()},\n",
    "    \"optim\": optimizer.state_dict()\n",
    "}\n",
    "torch.save(both_state, \"model_and_optimizer.pth\")\n",
    "print(\"Saved full checkpoint to: model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d201d7a",
   "metadata": {},
   "source": [
    "## 7) Load the full checkpoint and **resume training**\n",
    "We create fresh parameters and a fresh optimizer, then restore both from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81173587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New random params and optimizer (as if a fresh start)\n",
    "w2 = torch.randn(1, requires_grad=True)\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "optimizer2 = optim.AdamW([w2, b2], lr=1e-2, weight_decay=0.01)\n",
    "\n",
    "# Load checkpoint and restore\n",
    "ckpt = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "with torch.no_grad():\n",
    "    w2.copy_(ckpt[\"model\"][\"w\"])\n",
    "    b2.copy_(ckpt[\"model\"][\"b\"])\n",
    "optimizer2.load_state_dict(ckpt[\"optim\"])\n",
    "\n",
    "# Continue training for a few more steps\n",
    "for step in range(50):\n",
    "    optimizer2.zero_grad()\n",
    "    y_pred2 = x_batch * w2 + b2\n",
    "    loss2 = loss_fn(y_pred2, y_true)\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "print(\"Resumed w,b:\", w2.item(), b2.item())\n",
    "print(\"Loss after resume:\", float(loss2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548117f",
   "metadata": {},
   "source": [
    "## 8) Bonus — Deterministic generation/settings (no randomness)\n",
    "To make runs repeatable:\n",
    "- Fix random seeds (we did `torch.manual_seed(123)`).\n",
    "- Use `model.eval()` in real modules to turn off dropout.\n",
    "- For text generation: set **temperature=0**, disable **top-k / top-p**, and pick **argmax** at each step (greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33252e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate repeatable prediction with fixed weights\n",
    "torch.manual_seed(123)      # seed fixed\n",
    "x_demo = torch.tensor([[0.25], [0.75]])\n",
    "with torch.no_grad():\n",
    "    y_demo = x_demo * w2 + b2   # deterministic given w2,b2 and fixed seed above\n",
    "print(\"Deterministic demo outputs:\", y_demo.flatten().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
