{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fd130b",
   "metadata": {},
   "source": [
    "# Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bfd356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F  # Used for softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e42d-56a3-44d9-bc2e-b8405a629739",
   "metadata": {},
   "source": [
    "#### Below are the input embeddings where each word/friend has some numerical meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b791b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the input (4 friends, each with 3 features)\n",
    "# Each row represents one friend (or word in a sentence)\n",
    "friends = torch.tensor([\n",
    "    [0.2, 0.4, 0.6],   # Alice\n",
    "    [0.1, 0.3, 0.5],   # Bob\n",
    "    [0.9, 0.1, 0.2],   # Charlie\n",
    "    [0.7, 0.5, 0.9]    # Daisy\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeacf3a-2bfc-4ecd-9b03-8446511debc8",
   "metadata": {},
   "source": [
    "#### We create three trainable weight matrices for Query, Key, and Value.\n",
    "\n",
    "- W_Q: Learns how to ask good questions\n",
    "- W_K: Learns how to describe what each word is about\n",
    "- W_V: Learns how to give useful information\n",
    "\n",
    "These are learned during training, its like the model figuring out how to be a better listener over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b723210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create trainable weights for Query (Q), Key (K), and Value (V)\n",
    "# These help the model learn how to focus on relevant information\n",
    "W_Q = torch.nn.Linear(3, 3, bias=False)\n",
    "W_K = torch.nn.Linear(3, 3, bias=False)\n",
    "W_V = torch.nn.Linear(3, 3, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0111f07-7077-40eb-a686-4afdfbdd907f",
   "metadata": {},
   "source": [
    "#### We apply the weights to the inputs:\n",
    "\n",
    "- Q: What each word is asking\n",
    "- K: What each word is offering\n",
    "- V: The content each word holds\n",
    "\n",
    "For example, when the model sees \"it,\" the Query helps it ask, \"Who are you referring to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d5a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Q, K, V by applying the weights to the inputs\n",
    "Q = W_Q(friends)\n",
    "K = W_K(friends)\n",
    "V = W_V(friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018c3e6-df2c-4902-a01e-0acf0736bfa8",
   "metadata": {},
   "source": [
    "#### We compute how well each query matches each key.\n",
    "\n",
    "- Q . K^T: Measures similarity between words\n",
    "- Divide by √(dimension) to stabilize gradients (this is called scaled dot-product)\n",
    "\n",
    "This gives us raw attention scores stating, how much one word should attend to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339896f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate attention scores (similarity between Q and K)\n",
    "# Then scale them to keep values stable\n",
    "scores = torch.matmul(Q, K.T) / (Q.size(-1) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52296cca-71e1-40ef-bdd6-3cfc7e0c18f7",
   "metadata": {},
   "source": [
    "#### We convert raw scores into probabilities using softmax\n",
    "So now each word knows:\n",
    "\"I’ll listen 70% to ‘Alice’, 20% to ‘Bob’, 10% to others.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8031f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert scores to attention weights using softmax\n",
    "attention_weights = F.softmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64729389-d1d0-47a1-9dc2-d681eeea1dec",
   "metadata": {},
   "source": [
    "#### We take the weighted sum of Values.\n",
    "\n",
    "Each word combines other's content, based on how much attention it paid, this is the final contextual embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba43e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What the model understood:\n",
      " tensor([[ 0.5762, -0.3826, -0.2915],\n",
      "        [ 0.5757, -0.3829, -0.2920],\n",
      "        [ 0.5800, -0.3864, -0.2945],\n",
      "        [ 0.5799, -0.3821, -0.2896]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Use attention weights to get the final output (context-aware representations)\n",
    "output = torch.matmul(attention_weights, V)\n",
    "\n",
    "# Step 8: Print the result\n",
    "print(\"What the model understood:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07f203-814e-4c8d-a026-715d622c411b",
   "metadata": {},
   "source": [
    "#### What does this output mean?\n",
    "- Each row is the updated version of a word after it has \"listened\" to the others.\n",
    "- The model now understands each word not just on its own, but in the context of the whole sentence.\n",
    "- These vectors will be used in the next layers to help the model make smarter decisions, like predicting the next word or answering questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cd8fd-5a62-453c-8780-834a4aa35389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
