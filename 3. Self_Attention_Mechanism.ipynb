{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419afdb5",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism - Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62938f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2670a9935b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#with the next line of code, we are telling pytorch that whenever it generates random number, to do in the same way every time we run this code. \n",
    "#This makes the code repeatable and predictable, which is helpful for debugging or comparing results.\n",
    "\n",
    "torch.manual_seed(42)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0286327",
   "metadata": {},
   "source": [
    "## Step 1: Define Input Tokens\n",
    "- In the code below, we are representing words as numbers - embedding.\n",
    "- Each word in the sentence is turned into a **vector** (list of numbers) that describes its meaning (context) in a way that computer can understand. \n",
    "- Each of these is a 3D vector. Real LLMs use much higher dimensions, but for simplicity, we are keeping it small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dfa7e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors: tensor([[0.2000, 0.1000, 0.5000],\n",
      "        [0.9000, 0.1000, 0.3000],\n",
      "        [0.4000, 0.8000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "# Example: \"The cat ran\"\n",
    "inputs = torch.tensor([\n",
    "    [0.2, 0.1, 0.5],  # \"The\"\n",
    "    [0.9, 0.1, 0.3],  # \"cat\"\n",
    "    [0.4, 0.8, 0.2],  # \"ran\"\n",
    "])\n",
    "print(\"Input vectors:\", inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7924744",
   "metadata": {},
   "source": [
    "## Step 2: Compute Attention Scores\n",
    "\n",
    "- We measure similarity between words using dot products.\n",
    "- We are using the word \"ran\" as the focus word - the one we want to understand better using self-attention.\n",
    "- input[2] means give me the third word vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be3c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores: tensor([0.2600, 0.5000, 0.8400])\n"
     ]
    }
   ],
   "source": [
    "# We call this a query because in attention , the word is asking: \"Hey, which of you(words) are important for me\"\n",
    "query = inputs[2]  # focus on \"ran\"\n",
    "\n",
    "# We are creating an empty box that can store 3 numbers - one for each word in our sentence.\n",
    "attn_scores = torch.empty(inputs.shape[0])\n",
    "\n",
    "# This loop goes through each word vector and compares it with query vector using dot product.\n",
    "# Dot product tells us how similar the two vectors are. The higher the number, more related are the words to \"ran\"\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores[i] = torch.dot(query, x_i)\n",
    "\n",
    "print(\"Attention Scores:\", attn_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f369648-768a-46a5-8c5c-a5dc675a10ca",
   "metadata": {},
   "source": [
    "### The above output would mean \n",
    " - \"The\" - 0.2600 (not so important to \"ran\") \n",
    " - \"cat\" - 0.5000 (somewhat important) \n",
    " - \"ran\" - 0.8400 (very important to itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d6e47",
   "metadata": {},
   "source": [
    "## Step 3: Normalize Scores using Softmax\n",
    "\n",
    "Turn raw scores into probabilities that add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969cc6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Attention Weights: tensor([0.2465, 0.3133, 0.4402])\n",
      "Sum of weights: 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# Below line converts raw attention scores into probabilities - attention weights\n",
    "# Weights indicate how much focus each word would get.\n",
    "# Using softmax makes them all positive and they add up to 1.\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=0)\n",
    "print(\"Normalized Attention Weights:\", attn_weights\n",
    "\n",
    "# checks if the weights add up to 1\n",
    "print(\"Sum of weights:\", attn_weights.sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bdeeb-6206-4108-9de9-9d8ed8a73353",
   "metadata": {},
   "source": [
    "### The above output means that \n",
    "\n",
    "- \"The\" gets 26% attention \n",
    "- \"cat\" gets 31% attention \n",
    "- \"ran\" gets 44% attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cff04d",
   "metadata": {},
   "source": [
    "## Step 4: Compute Context Vector\n",
    "Each word contributes to the meaning of 'ran' based on its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94687586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector for 'ran': tensor([0.5074, 0.4081, 0.3053])\n"
     ]
    }
   ],
   "source": [
    "context_vec = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec += attn_weights[i] * x_i\n",
    "\n",
    "print(\"Context Vector for 'ran':\", context_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1eab6a",
   "metadata": {},
   "source": [
    "\n",
    "- Self-attention lets a word like **\"ran\"** look at **\"cat\"** and **\"the\"** to understand the full context.\n",
    "- We used dot product to get **scores**, softmax to get **weights**, and then blended them to get a **context vector**.\n",
    "- That's the basic idea behind self-attention used in LLMs like GPT! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e53639-dc80-4093-90b1-3a8b13c84954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
