{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37a9a24",
   "metadata": {},
   "source": [
    "# Multi-Head Attention -  Behind the Scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34111d-910b-4bf8-9ec2-4d045543931b",
   "metadata": {},
   "source": [
    "#### Step 1: Representing a Fake Sentence\n",
    "\n",
    "- Let's pretend we have 1 sentence made of 5 words.\n",
    "- Each word is represented as a list of 16 numbers (features).\n",
    "- This is how computers 'see' and process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5dec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.rand(1, 5, 16)  # shape: [batch_size, sequence_length, embedding_dim]\n",
    "print('Input shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb6f3b-a188-4abc-a7ed-d7f571782583",
   "metadata": {},
   "source": [
    "#### Step 2: Creating Query, Key, Value Vectors\n",
    "- Each word becomes 3 versions: Query, Key, and Value.\n",
    "- This helps the attention mechanism figure out what each word wants, what it offers, and what info it can share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6da2669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "W_q = torch.rand(16, 16)\n",
    "W_k = torch.rand(16, 16)\n",
    "W_v = torch.rand(16, 16)\n",
    "\n",
    "Q = x @ W_q\n",
    "K = x @ W_k\n",
    "V = x @ W_v\n",
    "\n",
    "print('Q shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77f0c0-6c05-47f3-814b-86ae59136cfa",
   "metadata": {},
   "source": [
    "#### Step 3: Splitting into Multiple Heads\n",
    "- We use 4 attention heads.\n",
    "- Each head gets 4 features from each word (16 ÷ 4 = 4).\n",
    "- So we reshape and rearrange to prepare for multi-head processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965c489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape after split: torch.Size([1, 4, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 4\n",
    "head_dim = 16 // num_heads\n",
    "\n",
    "Q = Q.view(1, 5, num_heads, head_dim).transpose(1, 2)\n",
    "K = K.view(1, 5, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(1, 5, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print('Q shape after split:', Q.shape)  # [1, 4, 5, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad1f20-5bf4-452a-a92d-603132915579",
   "metadata": {},
   "source": [
    "#### Step 4: Attention Calculation for Each Head\n",
    "- For each head, we calculate how much attention to pay to each word.\n",
    "- Then, use that to mix values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b219b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output per head: torch.Size([1, 4, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "weights = torch.softmax(scores, dim=-1)\n",
    "attention_output = torch.matmul(weights, V)\n",
    "\n",
    "print('Attention output per head:', attention_output.shape)  # [1, 4, 5, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8f257-7a9f-4577-80e7-457139243e79",
   "metadata": {},
   "source": [
    "#### Step 5: Combine All Heads\n",
    "- Bring all heads back together into one tensor.\n",
    "- Each word becomes a 16-feature vector again (4 heads × 4 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c503806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined attention output: torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "attention_output = attention_output.transpose(1, 2).reshape(1, 5, 16)\n",
    "print('Combined attention output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd26a2-abc3-4800-b614-4e20a6938eb5",
   "metadata": {},
   "source": [
    "#### Step 6: Final Output Projection\n",
    "Just like finishing a painting, one last transformation to polish the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a5f4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([1, 5, 16])\n",
      "Final output: tensor([[[32.8207, 44.8129, 36.9854, 37.3850, 33.7823, 32.0725, 37.7987,\n",
      "          37.2566, 35.7764, 26.0753, 41.7246, 37.4823, 43.3969, 38.7887,\n",
      "          32.5871, 36.0374],\n",
      "         [32.7385, 44.7132, 36.9051, 37.3156, 33.7123, 32.0124, 37.7272,\n",
      "          37.1702, 35.7004, 26.0061, 41.6326, 37.3906, 43.3078, 38.6993,\n",
      "          32.5240, 35.9688],\n",
      "         [32.7951, 44.7773, 36.9601, 37.3581, 33.7594, 32.0519, 37.7721,\n",
      "          37.2325, 35.7563, 26.0558, 41.6965, 37.4553, 43.3652, 38.7631,\n",
      "          32.5718, 36.0132],\n",
      "         [32.7404, 44.7106, 36.8987, 37.3192, 33.7165, 32.0080, 37.7316,\n",
      "          37.1678, 35.7139, 26.0038, 41.6335, 37.3912, 43.3118, 38.7009,\n",
      "          32.5341, 35.9692],\n",
      "         [32.7122, 44.6647, 36.8760, 37.2830, 33.6825, 31.9878, 37.6914,\n",
      "          37.1457, 35.6705, 25.9867, 41.5982, 37.3578, 43.2631, 38.6705,\n",
      "          32.5098, 35.9370]]])\n"
     ]
    }
   ],
   "source": [
    "W_o = torch.rand(16, 16)\n",
    "output = attention_output @ W_o\n",
    "\n",
    "print('Final output shape:', output.shape)\n",
    "print('Final output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b7050-4c05-4850-8901-7f1b310a53ca",
   "metadata": {},
   "source": [
    "### Meaning of the Output\n",
    "\n",
    "- **Shape `[1, 5, 16]`** → 1 sentence, 5 words, each now represented by 16 features.\n",
    "- **Values** --> These numbers are the new, richer meaning of each word after attention, capturing context and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe6ba5-5295-43e6-9022-b50903a522d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
