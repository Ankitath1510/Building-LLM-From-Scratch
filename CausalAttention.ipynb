{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbf74d1",
   "metadata": {},
   "source": [
    "# Causal Attention - Behind the Scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b450a-9f1c-4efb-8048-85d1cf49a739",
   "metadata": {},
   "source": [
    "#### Step 1. Import the tools we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9dc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  # For softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d54d0-bef8-4e1c-aa06-b5210c7a1c68",
   "metadata": {},
   "source": [
    "#### Step 2. Fix the randomness so the numbers stay the same every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22d9248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16510a02510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dda96f-2783-429d-b8eb-eb53d51ae0d0",
   "metadata": {},
   "source": [
    "#### Step 3. Create a pretend sentence with 6 words, each word has 3 features\n",
    "Each word is stored as 3 numbers (kind of like how the model sees words as math)\n",
    "Imagine:\n",
    "- Row 1 = the word “The”\n",
    "- Row 2 = the word “cat”\n",
    "- Row 3 = the word “ran” … and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a735968",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(6, 3)  # Shape: [6 words, 3 features per word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557f9d0-dd6a-43d1-8dc7-84d80246fcb3",
   "metadata": {},
   "source": [
    "#### Step 4. Set up 3 layers to help each word:\n",
    "Create 3 magical layers that help each word do three things:\n",
    "- Ask a question (Query)\n",
    "- Compare with others (Key)\n",
    "- Share its information (Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c3237e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = torch.nn.Linear(3, 3, bias=False) # This helps the word ask “What am I looking for?”\n",
    "W_K = torch.nn.Linear(3, 3, bias=False) # This helps others answer “Here’s what I offer.”\n",
    "W_V = torch.nn.Linear(3, 3, bias=False) # This shares each word’s actual content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548fca0-9aa9-4c5a-8e1a-bee801dc934f",
   "metadata": {},
   "source": [
    "#### Step 5. Generate Query, Key, and Value matrices\n",
    "Use those layers to turn the 6 words into Query, Key, and Value versions of themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d08ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = W_Q(inputs) # What each word is asking about\n",
    "K = W_K(inputs) # What each word offers\n",
    "V = W_V(inputs) # What each word carries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5231540-82cc-4993-9b65-d2d18835a7b0",
   "metadata": {},
   "source": [
    "#### Step 6. Compare how much attention each word gives to every other word\n",
    "Each word now looks at all others and scores how closely they match\n",
    "- Think of this like a \"who should I listen to?\" score\n",
    "- We divide by square root of 3 just to keep scores balanced\n",
    "###### The result is a 6x6 table:\n",
    "- Row 1 = how much word 1 listens to others\n",
    "- Row 2 = how much word 2 listens to others, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8ea6d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0640, 0.0679, 0.0194, 0.0903, 0.0184, 0.0417],\n",
      "        [0.0842, 0.1372, 0.0323, 0.1545, 0.0290, 0.0893],\n",
      "        [0.0259, 0.0333, 0.0087, 0.0410, 0.0079, 0.0209],\n",
      "        [0.0819, 0.1269, 0.0306, 0.1449, 0.0283, 0.0831],\n",
      "        [0.0528, 0.0541, 0.0156, 0.0738, 0.0138, 0.0315],\n",
      "        [0.0950, 0.1266, 0.0323, 0.1542, 0.0284, 0.0785]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(Q, K.T) / (K.shape[-1] ** 0.5)\n",
    "print (attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2656b58-4eb3-4b40-b34e-c8629b248117",
   "metadata": {},
   "source": [
    "#### Step 7. Build the mask to block 'future words'\n",
    "Now comes the important part: the “no cheating” rule!\n",
    "- We don’t want the model to look ahead in the sentence.\n",
    "- So we build a “lower triangular mask” , just a fancy term for:\n",
    "\n",
    "###### A 6x6 grid like this:\n",
    "##### [1 0 0 0 0 0]\n",
    "##### [1 1 0 0 0 0]\n",
    "##### [1 1 1 0 0 0]\n",
    "##### [1 1 1 1 0 0]\n",
    "##### [1 1 1 1 1 0]\n",
    "##### [1 1 1 1 1 1]\n",
    "\n",
    "##### This tells each word: “You can only look at yourself and words before you.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36c52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(attn_scores.shape[0], attn_scores.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca95d7-3601-4a27-907c-e321d2b06eff",
   "metadata": {},
   "source": [
    "#### Step 8. Apply the mask so attention can't go to future tokens\n",
    "- We’ll block all the 0s by replacing their scores with -infinity\n",
    "- This way, when we do softmax next, the model will completely ignore those blocked words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef454713",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_scores = attn_scores.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4883d-5d78-4c64-886a-2a83ec5614b5",
   "metadata": {},
   "source": [
    "#### Step 9. Convert the masked scores into attention weights (probabilities)\n",
    "- Use softmax to turn the scores into probabilities (like “how much attention should I give?”)\n",
    "- Now each row adds up to 1, it’s like slicing a pie and giving bigger slices to more important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25ccc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(masked_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c896657-96f4-4ca5-9401-4603a4091088",
   "metadata": {},
   "source": [
    "#### Step 10. Combine values using the attention weights\n",
    "- Multiply the attention weights with the Value matrix\n",
    "- This gives us the final answer: each word has now combined information from the words it listened to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a229867",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.matmul(attn_weights, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6aac6e-2200-422c-89e8-38aebf59c336",
   "metadata": {},
   "source": [
    "#### Step 11. Show the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "880ee80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final context-aware word embeddings:\n",
      " tensor([[-0.3325, -0.1223,  0.2555],\n",
      "        [-0.5215, -0.1879,  0.1063],\n",
      "        [-0.3994, -0.1458,  0.0869],\n",
      "        [-0.4794, -0.1667,  0.0904],\n",
      "        [-0.4201, -0.1554,  0.0910],\n",
      "        [-0.4472, -0.1731,  0.0766]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final context-aware word embeddings:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92934",
   "metadata": {},
   "source": [
    "### This is how models like GPT generate text one word at a time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f6e02-0020-4364-972b-8b6070fa7587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
